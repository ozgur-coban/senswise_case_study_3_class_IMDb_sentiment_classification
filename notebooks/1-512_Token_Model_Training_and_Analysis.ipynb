{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8dEcWdSEm07"
      },
      "source": [
        "# Project: IMDb 3-Class Sentiment Analysis\n",
        "# Notebook : 512-Token Model Training and Analysis\n",
        "\n",
        "This notebook contains the complete workflow for the 512-token-length model:\n",
        "1.  **Data Preparation:** Loads the raw IMDb dataset, tokenizes all text to `max_length=512`, and saves the processed dataset to Google Drive.\n",
        "2.  **Model Training:** Fine-tunes the `bert-base-uncased` model on the 512-token data.\n",
        "3.  **Binary Evaluation:** Evaluates the trained model for binary accuracy.\n",
        "4.  **Phase 2 Analysis:** Sets up and runs the 3-class heuristic systems (Ratio, Logit, Weighting) on a 1,000-review sample to generate the final comparison report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py1W2GOeEm08"
      },
      "source": [
        "## 1. Setup: Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxGLIFJPEm08"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets accelerate torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYkxtLk9Em09"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from google.colab import drive\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm.auto import tqdm # For progress bars\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFwmtlO6Em0-"
      },
      "outputs": [],
      "source": [
        "# Helper function to format elapsed time\n",
        "def format_time(elapsed):\n",
        "    '''Takes a time in seconds and returns a string hh:mm:ss'''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP6ctyDOEm0-"
      },
      "source": [
        "## 2. Phase 1: Data Preparation (512-Token)\n",
        "\n",
        "This step loads the raw IMDb dataset, tokenizes all text to a `max_length` of 512, and saves the processed dataset to Google Drive to avoid re-computing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxvIYn0mEm0-"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access and save project files\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define path to save the processed data\n",
        "save_path = '/content/drive/My Drive/BERT_IMDB_Processed_512'\n",
        "\n",
        "# Load the raw 'imdb' dataset from Hugging Face\n",
        "print(\"Loading IMDb dataset...\")\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "print(\"Loading bert-base-uncased tokenizer...\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_function_512(examples):\n",
        "    \"\"\"Tokenizes text, pads, and truncates to the max BERT length (512).\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512 # Set to BERT maximum\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing and preprocessing dataset (max_length=512)...\")\n",
        "tokenized_imdb = imdb.map(preprocess_function_512, batched=True)\n",
        "\n",
        "\n",
        "tokenized_imdb = tokenized_imdb.remove_columns([\"text\"])\n",
        "tokenized_imdb = tokenized_imdb.rename_column(\"label\", \"labels\")\n",
        "tokenized_imdb.set_format(\"torch\")\n",
        "print(\"Preprocessing complete.\")\n",
        "\n",
        "# Save the processed dataset to Google Drive\n",
        "print(f\"Saving tokenized dataset to: {save_path}\")\n",
        "tokenized_imdb.save_to_disk(save_path)\n",
        "\n",
        "print(f\"--- Successfully saved to Google Drive! ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l4owpSoEm1A"
      },
      "source": [
        "## 3. Phase 1: Model Training (512-Token)\n",
        "\n",
        "This section loads the pre-processed 512-token data, defines the training parameters, and runs the fine-tuning loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_viWBLMEm1A"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define Key Parameters ---\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 4\n",
        "LEARNING_RATE = 3e-6\n",
        "EPOCHS = 2\n",
        "\n",
        "# Define paths\n",
        "data_load_path = '/content/drive/My Drive/BERT_IMDB_Processed_512'\n",
        "model_save_path = '/content/drive/My Drive/BERT_IMDB_Model_Trained_512'\n",
        "\n",
        "# --- 2. Load Data from Drive ---\n",
        "print(f\"Loading processed data from: {data_load_path}\")\n",
        "tokenized_imdb = load_from_disk(data_load_path)\n",
        "\n",
        "# --- 3. Set Up DataLoaders ---\n",
        "print(\"Setting up DataLoaders...\")\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_imdb[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_imdb[\"test\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "print(\"DataLoaders are ready.\")\n",
        "print(f\"Using Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Training batches: {len(train_dataloader)}\")\n",
        "\n",
        "# --- 4. Define the Model ---\n",
        "print(\"Loading BertForSequenceClassification model...\")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2 # Binary classification\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model moved to {device}\")\n",
        "\n",
        "# --- 5. Set up Training Parameters ---\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "# --- 6. The Training Loop ---\n",
        "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
        "\n",
        "for epoch_i in range(0, EPOCHS):\n",
        "\n",
        "    print(f\"\\n======== Epoch {epoch_i + 1} / {EPOCHS} ========\")\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train() # training mode\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n",
        "\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        result = model(\n",
        "            b_input_ids,\n",
        "            token_type_ids=None,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        loss = result.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
        "    print(f\"  Training epoch took: {training_time}\")\n",
        "\n",
        "print(\"\\n--- Training complete! ---\")\n",
        "\n",
        "# --- 7. Save the trained model and tokenizer to your Drive ---\n",
        "print(f\"Saving model to {model_save_path}\")\n",
        "model.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Saving tokenizer to {model_save_path}...\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(\"Model and tokenizer saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0jHUphnEm1A"
      },
      "source": [
        "## 4. Phase 1: Binary Model Evaluation (512-Token)\n",
        "\n",
        "This section evaluates the 512-token model on the 25,000-review test set to get its baseline accuracy, precision, and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DR_Mhp5Em1A"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define Key Parameters ---\n",
        "EVAL_BATCH_SIZE = 128 # Use a larger batch size for (faster) evaluation\n",
        "NUM_WORKERS = 8\n",
        "PREFETCH_FACTOR = 4\n",
        "\n",
        "# --- 2. Define Paths ---\n",
        "data_load_path = '/content/drive/My Drive/BERT_IMDB_Processed_512'\n",
        "model_load_path = '/content/drive/My Drive/BERT_IMDB_Model_Trained_512'\n",
        "\n",
        "# --- 3. Load Model and Tokenizer ---\n",
        "print(f\"Loading trained model from: {model_load_path}\")\n",
        "model = BertForSequenceClassification.from_pretrained(model_load_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_load_path)\n",
        "\n",
        "# --- 4. Load Test Data ---\n",
        "print(f\"Loading processed data from: {data_load_path}\")\n",
        "tokenized_imdb = load_from_disk(data_load_path)\n",
        "\n",
        "# --- 5. Create OPTIMIZED Test DataLoader ---\n",
        "print(\"Setting up optimized Test DataLoader...\")\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_imdb[\"test\"],\n",
        "    batch_size=EVAL_BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=PREFETCH_FACTOR\n",
        ")\n",
        "print(f\"Test DataLoader is ready. Batch size: {EVAL_BATCH_SIZE}\")\n",
        "\n",
        "# --- 6. Set Up Evaluation ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval() # Put model in evaluation mode\n",
        "\n",
        "# Apply PyTorch 2.0 compile for faster inference\n",
        "print(\"Compiling model with torch.compile()... (This may take a moment)\")\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "    print(\"Model compiled successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"torch.compile() failed: {e}. Continuing without compilation.\")\n",
        "\n",
        "print(f\"Model moved to {device}. Starting evaluation...\")\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# --- 7. Optimized Evaluation Loop ---\n",
        "print(\"Starting evaluation...\")\n",
        "t0 = time.time()\n",
        "\n",
        "# torch.no_grad() disables gradient calculation, saving memory and VRAM\n",
        "with torch.no_grad():\n",
        "  # torch.cuda.amp.autocast() runs inference in faster FP16\n",
        "  with torch.cuda.amp.autocast():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(b_input_ids,\n",
        "                      token_type_ids=None,\n",
        "                      attention_mask=b_input_mask,\n",
        "                      return_dict=True)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        # Get the final prediction (0 or 1)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "eval_time = format_time(time.time() - t0)\n",
        "print(f\"Evaluation complete. Total time: {eval_time}\")\n",
        "\n",
        "# --- 8. Display Results ---\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=[\"Negative (0)\", \"Positive (1)\"])\n",
        "\n",
        "print(\"\\n--- Phase 1: Binary Classification Report (512-Token Model) ---\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b2qAw-ZEm1C"
      },
      "source": [
        "## 5. Phase 2: 3-Class System Comparison (512-Token Model)\n",
        "\n",
        "This section defines all three \"mixed\" classification systems and runs them on a 1,000-review sample to compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGSWYWrFEm1C"
      },
      "outputs": [],
      "source": [
        "# --- 1. Setup: NLTK and Load Model ---\n",
        "\n",
        "\n",
        "print(\"Downloading NLTK sentencizer (punkt)...\")\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"NLTK ready.\")\n",
        "\n",
        "# --- 2. Define Helper Functions for Model Prediction ---\n",
        "TOKENIZER_MAX_LENGTH = 512 # Set to 512 for this model\n",
        "\n",
        "# Helper for Systems 1 (Ratio) and 3 (Weighting)\n",
        "def predict_sentence_sentiment_binary(sentence_text):\n",
        "    \"\"\"Feeds a single sentence to the binary model and returns 0 or 1.\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        sentence_text, return_tensors=\"pt\", truncation=True,\n",
        "        padding=\"max_length\", max_length=TOKENIZER_MAX_LENGTH\n",
        "    )\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1)\n",
        "    return prediction.cpu().item()\n",
        "\n",
        "# Helper for System 2 (Logit)\n",
        "def get_sentence_logits(sentence_text):\n",
        "    \"\"\"Feeds a single sentence to the binary model and returns its raw logits.\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        sentence_text, return_tensors=\"pt\", truncation=True,\n",
        "        padding=\"max_length\", max_length=TOKENIZER_MAX_LENGTH\n",
        "    )\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask, return_dict=True)\n",
        "    return outputs.logits.squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "# --- 3. Define All Three Classification Systems ---\n",
        "\n",
        "# SYSTEM 1: Simple Ratio\n",
        "def classify_review_ratio(full_review_text, pos_threshold=0.7, neg_threshold=0.7):\n",
        "    \"\"\"Classifies based on the ratio of positive/negative sentences.\"\"\"\n",
        "    sentences = sent_tokenize(full_review_text)\n",
        "    # Classify short reviews based on the whole text\n",
        "    if len(sentences) < 3:\n",
        "        pred = predict_sentence_sentiment_binary(full_review_text)\n",
        "        return \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
        "\n",
        "    sentence_predictions = [predict_sentence_sentiment_binary(s) for s in sentences]\n",
        "    if not sentence_predictions: return \"NEUTRAL\"\n",
        "\n",
        "    num_sentences = len(sentence_predictions)\n",
        "    num_positive = sum(sentence_predictions)\n",
        "    positive_ratio = num_positive / num_sentences\n",
        "\n",
        "    # Apply threshold logic\n",
        "    if positive_ratio >= pos_threshold: return \"POSITIVE\"\n",
        "    elif (1 - positive_ratio) >= neg_threshold: return \"NEGATIVE\"\n",
        "    else: return \"MIXED\"\n",
        "\n",
        "# SYSTEM 2: Logit-Based \"Neutral Zone\"\n",
        "def classify_review_logit(full_review_text, neutral_threshold=1.0, min_sentences=2):\n",
        "    \"\"\"Classifies based on confidence (logits) and co-occurrence.\"\"\"\n",
        "    sentences = sent_tokenize(full_review_text)\n",
        "    if not sentences: return \"NEUTRAL\"\n",
        "\n",
        "    confident_pos_count = 0\n",
        "    confident_neg_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        logits = get_sentence_logits(sentence)\n",
        "        neg_score, pos_score = logits[0], logits[1]\n",
        "        score_difference = abs(pos_score - neg_score)\n",
        "\n",
        "\n",
        "        if score_difference >= neutral_threshold:\n",
        "            if pos_score > neg_score: confident_pos_count += 1\n",
        "            else: confident_neg_count += 1\n",
        "\n",
        "    # Rule: Must have at least 2 of each to be \"Mixed\"\n",
        "    if confident_pos_count >= min_sentences and confident_neg_count >= min_sentences: return \"MIXED\"\n",
        "    elif confident_pos_count > 0 and confident_neg_count == 0: return \"POSITIVE\"\n",
        "    elif confident_neg_count > 0 and confident_pos_count == 0: return \"NEGATIVE\"\n",
        "    else:\n",
        "        if confident_pos_count > confident_neg_count: return \"POSITIVE\"\n",
        "        elif confident_neg_count > confident_pos_count: return \"NEGATIVE\"\n",
        "        else: return \"NEUTRAL\"\n",
        "\n",
        "# SYSTEM 3: Positional Weighting + Co-occurrence\n",
        "def classify_review_weighting(full_review_text, positional_weight=2,\n",
        "                            mixed_pos_threshold=2, mixed_neg_threshold=2):\n",
        "    \"\"\"Classifies based on weighted co-occurrence, giving 2x weight to first/last sentences.\"\"\"\n",
        "    sentences = sent_tokenize(full_review_text)\n",
        "    if not sentences: return \"NEUTRAL\"\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "    weighted_pos_score = 0\n",
        "    weighted_neg_score = 0\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        current_weight = 1\n",
        "        # Apply 2x weight to first and last sentence\n",
        "        if i == 0 or i == (num_sentences - 1):\n",
        "            current_weight = positional_weight\n",
        "\n",
        "        prediction = predict_sentence_sentiment_binary(sentence)\n",
        "        if prediction == 1: weighted_pos_score += current_weight\n",
        "        else: weighted_neg_score += current_weight\n",
        "\n",
        "\n",
        "    if (weighted_pos_score >= mixed_pos_threshold and\n",
        "        weighted_neg_score >= mixed_neg_threshold): return \"MIXED\"\n",
        "    elif weighted_pos_score > weighted_neg_score: return \"POSITIVE\"\n",
        "    elif weighted_neg_score > weighted_pos_score: return \"NEGATIVE\"\n",
        "    else: return \"NEUTRAL\"\n",
        "\n",
        "\n",
        "# --- 4. Load Raw Test Data and Run Comparison ---\n",
        "print(\"\\nLoading original (non-tokenized) IMDb test set...\")\n",
        "imdb_raw = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "SAMPLE_SIZE = 1000\n",
        "test_reviews = imdb_raw['test'].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
        "\n",
        "results = []\n",
        "true_labels_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "\n",
        "print(f\"Running all 3 systems on {len(test_reviews)} test reviews...\")\n",
        "t0 = time.time()\n",
        "\n",
        "for review in tqdm(test_reviews, desc=\"Analyzing Test Set\"):\n",
        "    text = review['text']\n",
        "    true_label = true_labels_map[review['label']]\n",
        "\n",
        "    # Run all three classifiers on the same review\n",
        "    pred_1 = classify_review_ratio(text)\n",
        "    pred_2 = classify_review_logit(text)\n",
        "    pred_3 = classify_review_weighting(text)\n",
        "\n",
        "    results.append({\n",
        "        \"true_label\": true_label,\n",
        "        \"system_1_ratio\": pred_1,\n",
        "        \"system_2_logit\": pred_2,\n",
        "        \"system_3_weighting\": pred_3,\n",
        "        \"text\": text\n",
        "    })\n",
        "\n",
        "total_time = format_time(time.time() - t0)\n",
        "print(f\"Analysis complete. Total time: {total_time}\")\n",
        "\n",
        "# --- 5. Generate and Print Comparison Report ---\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" COMPARISON 1: OVERALL PREDICTION DISTRIBUTION (Sample size={SAMPLE_SIZE})\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nSystem 1 (Ratio) Distribution:\")\n",
        "print(df['system_1_ratio'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "print(\"\\nSystem 2 (Logit/Neutral) Distribution:\")\n",
        "print(df['system_2_logit'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "print(\"\\nSystem 3 (Weighting) Distribution:\")\n",
        "print(df['system_3_weighting'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" COMPARISON 2: AGREEMENT WITH TRUE LABELS (Sample size={SAMPLE_SIZE})\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nSystem 1 (Ratio) vs. True Labels:\")\n",
        "print(pd.crosstab(df['true_label'], df['system_1_ratio']))\n",
        "print(\"\\nSystem 2 (Logit/Neutral) vs. True Labels:\")\n",
        "print(pd.crosstab(df['true_label'], df['system_2_logit']))\n",
        "print(\"\\nSystem 3 (Weighting) vs. True Labels:\")\n",
        "print(pd.crosstab(df['true_label'], df['system_3_weighting']))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" COMPARISON 3: HARD ERROR COUNT (Sample size={SAMPLE_SIZE})\")\n",
        "print(\"=\"*50)\n",
        "s1_errors = len(df[(df['true_label'] == \"POSITIVE\") & (df['system_1_ratio'] == \"NEGATIVE\")]) + \\\n",
        "            len(df[(df['true_label'] == \"NEGATIVE\") & (df['system_1_ratio'] == \"POSITIVE\")])\n",
        "print(f\"System 1 (Ratio) Hard Errors: {s1_errors}\")\n",
        "s2_errors = len(df[(df['true_label'] == \"POSITIVE\") & (df['system_2_logit'] == \"NEGATIVE\")]) + \\\n",
        "            len(df[(df['true_label'] == \"NEGATIVE\") & (df['system_2_logit'] == \"POSITIVE\")])\n",
        "print(f\"System 2 (Logit) Hard Errors: {s2_errors}\")\n",
        "s3_errors = len(df[(df['true_label'] == \"POSITIVE\") & (df['system_3_weighting'] == \"NEGATIVE\")]) + \\\n",
        "            len(df[(df['true_label'] == \"NEGATIVE\") & (df['system_3_weighting'] == \"POSITIVE\")])\n",
        "print(f\"System 3 (Weighting) Hard Errors: {s3_errors}\")\n",
        "\n",
        "# --- 6. Save Full Results to CSV for Qualitative Analysis ---\n",
        "results_save_path = f'/content/drive/My Drive/BERT_IMDB_Model_Trained_512/sample_comparison_results_512.csv'\n",
        "print(f\"\\nSaving sample results to {results_save_path} for manual inspection...\")\n",
        "df.to_csv(results_save_path, index=False)\n",
        "print(\"Done.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}