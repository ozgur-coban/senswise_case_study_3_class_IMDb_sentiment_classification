{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzgTrS8UMQ7A"
      },
      "source": [
        "# Project: IMDb 3-Class Sentiment Analysis\n",
        "# Notebook: Analysis and Results\n",
        "\n",
        "This notebook demonstrates the final, trained 3-class sentiment analysis systems.\n",
        "\n",
        "It will:\n",
        "1.  Download the two pre-trained BERT models (256-token and 512-token) from public Google Drive links.\n",
        "2.  Load the **512-token model** and run the **1,000-sample 3-system comparison**.\n",
        "3.  Load the **256-token model** and run the **10,000-sample validation** of the winning heuristic (System 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5uc-74QMQ7B"
      },
      "source": [
        "## Final Project Results (Summary)\n",
        "\n",
        "This notebook reproduces the key findings of the project.\n",
        "\n",
        "### 1. Binary Model Performance (Phase 1)\n",
        "* **256-Token Model Accuracy:** 92.0%\n",
        "* **512-Token Model Accuracy:** 93.0%\n",
        "\n",
        "### 2. 3-System Heuristic Comparison (1,000-Sample Test, 512-Token Model)\n",
        "\n",
        "The 'Hard Error' count measures complete misclassifications (e.g., 'Positive' as 'Negative').\n",
        "\n",
        "* **System 1 (Ratio) Hard Errors:** 27\n",
        "* **System 2 (Logit) Hard Errors:** 31\n",
        "* **System 3 (Weighting) Hard Errors:** **8 (Winner)**\n",
        "\n",
        "### 3. System 3 Validation (10,000-Sample Test, 256-Token Model)\n",
        "\n",
        "The winning heuristic (System 3) was validated on a larger sample using the more efficient 256-token model.\n",
        "\n",
        "* **Total Hard Errors:** 126\n",
        "* **Final Hard Error Rate:** **1.26%**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculated Results:\n",
        "\n",
        "Analysis Data (1,000-Sample 3-System Comparison):\n",
        "https://drive.google.com/file/d/1qxyaRpJLQiE9UoxsO1MraKjSIST10lNI/view?usp=sharing\n",
        "\n",
        "Analysis Data (10,000-Sample Validation of System 3):\n",
        "https://drive.google.com/file/d/1UTiKFeBe31ZZUwi41xVKAPtS5Hv5dWG-/view?usp=sharing"
      ],
      "metadata": {
        "id": "S6ksQN8HMbw8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAsYh5YmMQ7C"
      },
      "source": [
        "---\n",
        "## 1. Setup: Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUjmyJYcMQ7C"
      },
      "outputs": [],
      "source": [
        "# Install all required libraries\n",
        "!pip install transformers datasets accelerate torch gdown pandas nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAIaKQCwMQ7D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Helper function to format elapsed time\n",
        "def format_time(elapsed):\n",
        "    '''Takes a time in seconds and returns a string hh:mm:ss'''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_fybONJMQ7D"
      },
      "source": [
        "## 2. Download and Unzip Pre-Trained Models\n",
        "\n",
        "This step downloads the two `.zip` files containing the trained models from public Google Drive links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbGx3lxiMQ7D"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "URL_256_MODEL = \"https://drive.google.com/file/d/1GOigFk5XDsUiju5AuTesWMS_lxp77N8u/view?usp=sharing\"\n",
        "\n",
        "\n",
        "URL_512_MODEL = \"https://drive.google.com/file/d/1kebIYqseo_2X7FeHnk64tXVpOMuyidro/view?usp=sharing\"\n",
        "\n",
        "# --- End of Action Required ---\n",
        "\n",
        "def download_and_unzip(url, zip_name, extract_path):\n",
        "    \"\"\"Downloads a zip file from a GDrive URL and unzips it.\"\"\"\n",
        "    print(f\"Downloading {zip_name}...\")\n",
        "    if \"drive.google.com\" in url:\n",
        "        file_id = url.split('/')[-2]\n",
        "        url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "    gdown.download(url, zip_name, quiet=False)\n",
        "    print(f\"Download complete. Unzipping to {extract_path}...\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    print(f\"Model successfully unzipped.\")\n",
        "    os.remove(zip_name) # Clean up the zip file\n",
        "\n",
        "# Download and unzip both models\n",
        "download_and_unzip(URL_256_MODEL, \"model_256.zip\", \"./model_256/\")\n",
        "download_and_unzip(URL_512_MODEL, \"model_512.zip\", \"./model_512/\")\n",
        "\n",
        "print(\"\\nAll models are downloaded and ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ_KxZR_MQ7E"
      },
      "source": [
        "## 3. Define the 3-Class Heuristic Systems\n",
        "\n",
        "These are the three systems (Ratio, Logit, Weighting) that will be tested. They rely on helper functions to get predictions from the currently loaded model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dqLSj2JMQ7E"
      },
      "outputs": [],
      "source": [
        "# --- 1. Setup: NLTK and Global Variables ---\n",
        "print(\"Downloading NLTK sentencizer (punkt)...\")\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"NLTK ready.\")\n",
        "\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TOKENIZER_MAX_LENGTH = 512\n",
        "\n",
        "# --- 2. Define Helper Functions for Model Prediction ---\n",
        "\n",
        "def predict_sentence_sentiment_binary(sentence_text):\n",
        "    \"\"\"Feeds a single sentence to the currently loaded binary model and returns 0 or 1.\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        sentence_text, return_tensors=\"pt\", truncation=True,\n",
        "        padding=\"max_length\", max_length=TOKENIZER_MAX_LENGTH\n",
        "    )\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1)\n",
        "    return prediction.cpu().item()\n",
        "\n",
        "def get_sentence_logits(sentence_text):\n",
        "    \"\"\"Feeds a single sentence to the currently loaded binary model and returns its raw logits.\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        sentence_text, return_tensors=\"pt\", truncation=True,\n",
        "        padding=\"max_length\", max_length=TOKENIZER_MAX_LENGTH\n",
        "    )\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask, return_dict=True)\n",
        "    return outputs.logits.squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "# --- 3. Define All Three Classification Systems ---\n",
        "\n",
        "# SYSTEM 1: Simple Ratio\n",
        "def classify_review_ratio(full_review_text, pos_threshold=0.7, neg_threshold=0.7):\n",
        "    \"\"\"Classifies based on the ratio of positive/negative sentences.\"\"\"\n",
        "    sentences = sent_tokenize(full_review_text)\n",
        "    if len(sentences) < 3:\n",
        "        pred = predict_sentence_sentiment_binary(full_review_text)\n",
        "        return \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
        "    sentence_predictions = [predict_sentence_sentiment_binary(s) for s in sentences]\n",
        "    if not sentence_predictions: return \"NEUTRAL\"\n",
        "    num_sentences = len(sentence_predictions)\n",
        "    num_positive = sum(sentence_predictions)\n",
        "    positive_ratio = num_positive / num_sentences\n",
        "    if positive_ratio >= pos_threshold: return \"POSITIVE\"\n",
        "    elif (1 - positive_ratio) >= neg_threshold: return \"NEGATIVE\"\n",
        "    else: return \"MIXED\"\n",
        "\n",
        "# SYSTEM 2: Logit-Based \"Neutral Zone\"\n",
        "def classify_review_logit(full_review_text, neutral_threshold=1.0, min_sentences=2):\n",
        "    \"\"\"Classifies based on confidence (logits) and co-occurrence.\"\"\"\n",
        "    sentences = sent_tokenize(full_review_text)\n",
        "    if not sentences: return \"NEUTRAL\"\n",
        "    confident_pos_count = 0\n",
        "    confident_neg_count = 0\n",
        "    for sentence in sentences:\n",
        "        logits = get_sentence_logits(sentence)\n",
        "        neg_score, pos_score = logits[0], logits[1]\n",
        "        score_difference = abs(pos_score - neg_score)\n",
        "        if score_difference >= neutral_threshold:\n",
        "            if pos_score > neg_score: confident_pos_count += 1\n",
        "            else: confident_neg_count += 1\n",
        "    if confident_pos_count >= min_sentences and confident_neg_count >= min_sentences: return \"MIXED\"\n",
        "    elif confident_pos_count > 0 and confident_neg_count == 0: return \"POSITIVE\"\n",
        "    elif confident_neg_count > 0 and confident_pos_count == 0: return \"NEGATIVE\"\n",
        "    else:\n",
        "        if confident_pos_count > confident_neg_count: return \"POSITIVE\"\n",
        "        elif confident_neg_count > confident_pos_count: return \"NEGATIVE\"\n",
        "        else: return \"NEUTRAL\"\n",
        "\n",
        "# SYSTEM 3: Positional Weighting + Co-occurrence\n",
        "def classify_review_weighting(full_review_text, positional_weight=2,\n",
        "                            mixed_pos_threshold=2, mixed_neg_threshold=2):\n",
        "    \"\"\"Classifies based on weighted co-occurrence, giving 2x weight to first/last sentences.\"\"\"\n",
        "    sentences = sent_tokenize(full_review_text)\n",
        "    if not sentences: return \"NEUTRAL\"\n",
        "    num_sentences = len(sentences)\n",
        "    weighted_pos_score = 0\n",
        "    weighted_neg_score = 0\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        current_weight = 1\n",
        "        if i == 0 or i == (num_sentences - 1):\n",
        "            current_weight = positional_weight\n",
        "        prediction = predict_sentence_sentiment_binary(sentence)\n",
        "        if prediction == 1: weighted_pos_score += current_weight\n",
        "        else: weighted_neg_score += current_weight\n",
        "    if (weighted_pos_score >= mixed_pos_threshold and\n",
        "        weighted_neg_score >= mixed_neg_threshold): return \"MIXED\"\n",
        "    elif weighted_pos_score > weighted_neg_score: return \"POSITIVE\"\n",
        "    elif weighted_neg_score > weighted_pos_score: return \"NEGATIVE\"\n",
        "    else: return \"NEUTRAL\"\n",
        "\n",
        "print(\"All 3 systems defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI4wWlhGMQ7F"
      },
      "source": [
        "## 4. Run Analysis 1: 3-System Comparison (1,000 Samples, 512-Token Model)\n",
        "\n",
        "This section loads the 512-token model and runs it on a 1,000-review sample to generate the first set of comparison tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GekCTvpuMQ7G"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load the 512-Token Model ---\n",
        "print(\"Loading 512-token model for analysis...\")\n",
        "model_load_path = \"./model_512/BERT_IMDB_Model_Trained_512\"\n",
        "TOKENIZER_MAX_LENGTH = 512\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_load_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_load_path)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "    print(\"Model compiled successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Continuing without compilation.\")\n",
        "\n",
        "# --- 2. Load Raw Test Data ---\n",
        "print(\"\\nLoading original (non-tokenized) IMDb test set...\")\n",
        "imdb_raw = load_dataset(\"imdb\")\n",
        "\n",
        "SAMPLE_SIZE = 1000\n",
        "test_reviews = imdb_raw['test'].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
        "\n",
        "results_1k = []\n",
        "true_labels_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "\n",
        "print(f\"Running all 3 systems on {len(test_reviews)} test reviews (512-token model)...\")\n",
        "t0 = time.time()\n",
        "\n",
        "# --- 3. Run Analysis Loop ---\n",
        "for review in tqdm(test_reviews, desc=\"Analyzing 1k Sample (512-model)\"):\n",
        "    text = review['text']\n",
        "    true_label = true_labels_map[review['label']]\n",
        "    pred_1 = classify_review_ratio(text)\n",
        "    pred_2 = classify_review_logit(text)\n",
        "    pred_3 = classify_review_weighting(text)\n",
        "    results_1k.append({\n",
        "        \"true_label\": true_label,\n",
        "        \"system_1_ratio\": pred_1,\n",
        "        \"system_2_logit\": pred_2,\n",
        "        \"system_3_weighting\": pred_3,\n",
        "        \"text\": text\n",
        "    })\n",
        "\n",
        "total_time = format_time(time.time() - t0)\n",
        "print(f\"Analysis complete. Total time: {total_time}\")\n",
        "\n",
        "# --- 4. Generate and Print Comparison Report ---\n",
        "df_1k = pd.DataFrame(results_1k)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" 512-MODEL: 3-SYSTEM COMPARISON (Sample size={SAMPLE_SIZE})\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nSystem 1 (Ratio) Distribution:\")\n",
        "print(df_1k['system_1_ratio'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "print(\"\\nSystem 2 (Logit/Neutral) Distribution:\")\n",
        "print(df_1k['system_2_logit'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "print(\"\\nSystem 3 (Weighting) Distribution:\")\n",
        "print(df_1k['system_3_weighting'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" 512-MODEL: AGREEMENT WITH TRUE LABELS (Sample size={SAMPLE_SIZE})\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nSystem 1 (Ratio) vs. True Labels:\")\n",
        "print(pd.crosstab(df_1k['true_label'], df_1k['system_1_ratio']))\n",
        "print(\"\\nSystem 2 (Logit/Neutral) vs. True Labels:\")\n",
        "print(pd.crosstab(df_1k['true_label'], df_1k['system_2_logit']))\n",
        "print(\"\\nSystem 3 (Weighting) vs. True Labels:\")\n",
        "print(pd.crosstab(df_1k['true_label'], df_1k['system_3_weighting']))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" 512-MODEL: HARD ERROR COUNT (Sample size={SAMPLE_SIZE})\")\n",
        "print(\"=\"*50)\n",
        "s1_errors = len(df_1k[(df_1k['true_label'] == \"POSITIVE\") & (df_1k['system_1_ratio'] == \"NEGATIVE\")]) + \\\n",
        "            len(df_1k[(df_1k['true_label'] == \"NEGATIVE\") & (df_1k['system_1_ratio'] == \"POSITIVE\")])\n",
        "print(f\"System 1 (Ratio) Hard Errors: {s1_errors}\")\n",
        "s2_errors = len(df_1k[(df_1k['true_label'] == \"POSITIVE\") & (df_1k['system_2_logit'] == \"NEGATIVE\")]) + \\\n",
        "            len(df_1k[(df_1k['true_label'] == \"NEGATIVE\") & (df_1k['system_2_logit'] == \"POSITIVE\")])\n",
        "print(f\"System 2 (Logit) Hard Errors: {s2_errors}\")\n",
        "s3_errors = len(df_1k[(df_1k['true_label'] == \"POSITIVE\") & (df_1k['system_3_weighting'] == \"NEGATIVE\")]) + \\\n",
        "            len(df_1k[(df_1k['true_label'] == \"NEGATIVE\") & (df_1k['system_3_weighting'] == \"POSITIVE\")])\n",
        "print(f\"System 3 (Weighting) Hard Errors: {s3_errors}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejmeg9wIMQ7G"
      },
      "source": [
        "## 5. Run Analysis 2: System 3 Validation (10,000 Samples, 256-Token Model)\n",
        "\n",
        "This section validates the winning heuristic (System 3) on a larger 10,000-review sample using the faster 256-token model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXLP770wMQ7H"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load the 256-Token Model ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading 256-token model for validation...\")\n",
        "model_load_path = \"./model_256/BERT_IMDB_Model_Trained_256_FP16_Optimized\"\n",
        "TOKENIZER_MAX_LENGTH = 256 # Switch to 256\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_load_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_load_path)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "    print(\"Model compiled successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Continuing without compilation.\")\n",
        "\n",
        "# --- 2. Load Raw Test Data ---\n",
        "print(\"\\nLoading original (non-tokenized) IMDb test set...\")\n",
        "imdb_raw = load_dataset(\"imdb\")\n",
        "\n",
        "SAMPLE_SIZE = 10000\n",
        "test_reviews = imdb_raw['test'].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
        "\n",
        "results_10k = []\n",
        "true_labels_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "\n",
        "print(f\"Running System 3 on {len(test_reviews)} test reviews (256-token model)...\")\n",
        "t0 = time.time()\n",
        "\n",
        "# --- 3. Run Analysis Loop (System 3 Only) ---\n",
        "for review in tqdm(test_reviews, desc=\"Analyzing 10k Sample (256-model)\"):\n",
        "    text = review['text']\n",
        "    true_label = true_labels_map[review['label']]\n",
        "    pred_3 = classify_review_weighting(text)\n",
        "    results_10k.append({\n",
        "        \"true_label\": true_label,\n",
        "        \"system_3_weighting\": pred_3,\n",
        "        \"text\": text\n",
        "    })\n",
        "\n",
        "total_time = format_time(time.time() - t0)\n",
        "print(f\"Analysis complete. Total time: {total_time}\")\n",
        "\n",
        "# --- 4. Generate and Print Comparison Report ---\n",
        "df_10k = pd.DataFrame(results_10k)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" 256-MODEL: SYSTEM 3 VALIDATION (Sample size={SAMPLE_SIZE})\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nSystem 3 (Weighting) Distribution:\")\n",
        "print(df_10k['system_3_weighting'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
        "\n",
        "print(\"\\nSystem 3 (Weighting) vs. True Labels:\")\n",
        "print(pd.crosstab(df_10k['true_label'], df_10k['system_3_weighting']))\n",
        "\n",
        "s3_errors_10k = len(df_10k[(df_10k['true_label'] == \"POSITIVE\") & (df_10k['system_3_weighting'] == \"NEGATIVE\")]) + \\\n",
        "            len(df_10k[(df_10k['true_label'] == \"NEGATIVE\") & (df_10k['system_3_weighting'] == \"POSITIVE\")])\n",
        "print(f\"\\nSystem 3 (Weighting) Hard Errors: {s3_errors_10k}\")\n",
        "print(f\"Hard Error Rate: {s3_errors_10k / SAMPLE_SIZE * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBjLtVAOMQ7H"
      },
      "source": [
        "## 6. Save Final CSVs\n",
        "\n",
        "This cell saves the DataFrames generated above as `.csv` files for local use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTw_qbU4MQ7H"
      },
      "outputs": [],
      "source": [
        "print(\"Saving final analysis CSVs...\")\n",
        "\n",
        "# Save the 1k 3-system comparison\n",
        "results_1k_path = 'sample_comparison_results_1k_512model.csv'\n",
        "df_1k.to_csv(results_1k_path, index=False)\n",
        "print(f\"Saved 1k sample results to {results_1k_path}\")\n",
        "\n",
        "# Save the 10k System 3 validation\n",
        "results_10k_path = 'final_system3_validation_10k_256model.csv'\n",
        "df_10k.to_csv(results_10k_path, index=False)\n",
        "print(f\"Saved 10k validation results to {results_10k_path}\")\n",
        "\n",
        "print(\"Done.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}